{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#code for connecting notebook to google drive"
      ],
      "metadata": {
        "id": "nL-A4dM3nuLL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnXmeh8UmXTU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#code for scrapping movie-related data\n"
      ],
      "metadata": {
        "id": "s5EUncc9nowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_titles(url, letter):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    movies = set()\n",
        "\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith('/movie/') and href.endswith('.htm'):\n",
        "            # Extract the movie title from the URL and check the first letter\n",
        "            title = href.split('/')[-1].replace('.htm', '').replace('_', ' ')\n",
        "            if title.lower().startswith(letter):\n",
        "                movies.add(href)\n",
        "\n",
        "    return list(movies)"
      ],
      "metadata": {
        "id": "jCFUWwCpmZ4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_movie_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    #  extracting the movie title and year\n",
        "    title_year_element = soup.find('a', itemprop=\"url\", href=lambda href: href and \"/movie/\" in href)\n",
        "    if title_year_element:\n",
        "        title_year_text = title_year_element.get_text()\n",
        "        if '(' in title_year_text and ')' in title_year_text:\n",
        "            title = title_year_text.split('(')[0].strip()\n",
        "            year = title_year_text.split('(')[-1].split(')')[0]\n",
        "        else:\n",
        "            title = title_year_text.strip()\n",
        "            year = 'Unknown'\n",
        "    else:\n",
        "        title = 'Unknown'\n",
        "        year = 'Unknown'\n",
        "\n",
        "    # Extracting details from the table\n",
        "    details = {'Title': title, 'Year': year}\n",
        "    table = soup.find('table', class_='b1 allef w100p')\n",
        "    if table:\n",
        "        rows = table.find_all('tr')\n",
        "        for row in rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 2:  # Ensuring there are two columns\n",
        "                key = cells[0].get_text(strip=True).replace(':', '')\n",
        "                value = cells[1].get_text(strip=True)\n",
        "                if key and value:\n",
        "                    details[key] = value\n",
        "            if \"Lyricist:\" in cells[0].get_text():\n",
        "                details['Lyricist'] = cells[1].get_text(strip=True)\n",
        "\n",
        "    # Handling external links separately\n",
        "    external_links_elements = soup.find_all('a', itemprop=\"sameAs\")\n",
        "    external_links = [elem['href'] for elem in external_links_elements] if external_links_elements else []\n",
        "    details['External Links'] = ', '.join(external_links)\n",
        "\n",
        "    # Handling 'Watch Full Movie' link separately\n",
        "    watch_movie_element = soup.find(lambda tag: tag.name == \"td\" and \"Watch Full Movie:\" in tag.text)\n",
        "    watch_movie_link = watch_movie_element.find_next_sibling('td').find('a')['href'] if watch_movie_element and watch_movie_element.find_next_sibling('td') else 'Not Available'\n",
        "    details['Watch Full Movie'] = watch_movie_link\n",
        "\n",
        "    return details"
      ],
      "metadata": {
        "id": "ezYKcztNnlRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# alphabets = list(string.ascii_lowercase) #would work for all alphabets\n",
        "# for testing purposes\n",
        "alphabets = ['z']\n",
        "\n",
        "all_movie_details = []\n",
        "df_movies = pd.DataFrame()\n",
        "\n",
        "for letter in alphabets:\n",
        "    page = 1\n",
        "    previous_movies = None\n",
        "    while True:\n",
        "        url = f\"https://www.hindilyrics4u.com/movie/{letter}.php\" + (\"\" if page == 1 else f\"?page={page}\")\n",
        "        movies = get_movie_titles(url, letter)\n",
        "\n",
        "        # Check if new page content is same as previous or empty\n",
        "        if not movies or movies == previous_movies:\n",
        "            break\n",
        "        print(movies)\n",
        "\n",
        "        for movie_path in movies:\n",
        "\n",
        "            movie_url = \"https://www.hindilyrics4u.com\" + movie_path\n",
        "            print(movie_url)\n",
        "            movie_details = get_movie_details(movie_url)\n",
        "            all_movie_details.append(movie_details)\n",
        "\n",
        "        previous_movies = movies\n",
        "        page += 1\n",
        "\n",
        "\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df_movies = pd.DataFrame(all_movie_details)\n",
        "\n",
        "movies_csv_path = '/content/drive/MyDrive/Fall 2023/Applied Data Science/datasets/m3/movie_final123.csv'\n",
        "\n",
        "# Save to CSV\n",
        "\n",
        "df_movies.to_csv(movies_csv_path, index=False)\n",
        "print(\"Movies data saved to 'movie_final.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PFb3sbvPot7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#code for scrapping song-related data"
      ],
      "metadata": {
        "id": "FF9Laxfwol0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_artist_names(url, letter):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    artist_elements = soup.find_all('a', href=lambda href: href and \"/singer/\" in href)\n",
        "\n",
        "    artist_names = []\n",
        "    for element in artist_elements:\n",
        "        full_text = element.get_text()\n",
        "        name = full_text.split('(')[0].strip()  # Remove the count and strip any whitespace\n",
        "\n",
        "        # Ensure the name starts with the specified letter and is not just a single letter\n",
        "        if name.lower().startswith(letter.lower()) and len(name) > 1:\n",
        "            artist_names.append(name)\n",
        "\n",
        "    return artist_names"
      ],
      "metadata": {
        "id": "taKw5gTCooFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_songs_by_artist(base_url, artist_name):\n",
        "    page = 1\n",
        "    previous_songs = None\n",
        "    all_songs = []\n",
        "\n",
        "    artist_name_formatted = artist_name.replace(' ', '_').lower()\n",
        "\n",
        "    while True:\n",
        "        url = f\"{base_url}/{artist_name_formatted}.php\" + (\"\" if page == 1 else f\"?page={page}\")\n",
        "        print(url)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        song_elements = soup.find_all('tr', itemprop=\"track\")\n",
        "        current_songs = []\n",
        "\n",
        "        for element in song_elements:\n",
        "            song_title_element = element.find('span', itemprop=\"name\")\n",
        "            if song_title_element:\n",
        "                song_title = song_title_element.get_text()\n",
        "\n",
        "                rating_element = element.find('span', itemprop=\"ratingValue\")\n",
        "                rating = rating_element.get_text() if rating_element else 'Unknown'\n",
        "\n",
        "                votes_element = element.find('span', itemprop=\"ratingCount\")\n",
        "                votes = votes_element.get_text() if votes_element else 'Unknown'\n",
        "\n",
        "                movie_element = element.find('span', itemprop=\"inAlbum\")\n",
        "                movie_name = movie_element.get_text() if movie_element else 'Unknown'\n",
        "\n",
        "                current_songs.append({\n",
        "                    'Song Title': song_title,\n",
        "                    'Rating': rating,\n",
        "                    'Votes': votes,\n",
        "                    'Movie': movie_name\n",
        "                })\n",
        "\n",
        "        if not current_songs or current_songs == previous_songs:\n",
        "            break\n",
        "\n",
        "        all_songs.extend(current_songs)\n",
        "        previous_songs = current_songs\n",
        "        page += 1\n",
        "\n",
        "    return all_songs\n"
      ],
      "metadata": {
        "id": "0GE8Ygb6on6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Main script\n",
        "base_url = \"https://www.hindilyrics4u.com/singer/{}.php\"\n",
        "base_artist_url = \"https://www.hindilyrics4u.com/singer\"\n",
        "# alphabets = list(string.ascii_lowercase) #would work for all alphabets\n",
        "alphabets = ['z']  # for testing purposes\n",
        "\n",
        "all_artists = []\n",
        "\n",
        "\n",
        "\n",
        "for letter in alphabets:\n",
        "    page = 1\n",
        "    previous_names = None\n",
        "\n",
        "    while True:\n",
        "        url = base_url.format(letter) + (\"\" if page == 1 else f\"?page={page}\")\n",
        "        artist_names = get_artist_names(url, letter)\n",
        "\n",
        "        if not artist_names or artist_names == previous_names:\n",
        "            break\n",
        "\n",
        "        all_artists.extend(artist_names)\n",
        "        previous_names = artist_names\n",
        "        page += 1\n",
        "\n",
        "data = []\n",
        "\n",
        "for artist in all_artists:\n",
        "    songs = get_songs_by_artist(base_artist_url, artist.replace(' ', '_'))\n",
        "    for song in songs:\n",
        "        data.append({\n",
        "            'Artist': artist,\n",
        "            'Song Title': song['Song Title'],\n",
        "            'Rating': song['Rating'],\n",
        "            'Number of Votes': song['Votes'],\n",
        "            'Movie Title': song['Movie']\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('/content/drive/MyDrive/Fall 2023/Applied Data Science/datasets/songs_final.csv', index=False)\n"
      ],
      "metadata": {
        "id": "XtCpcM7Yo_oG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}